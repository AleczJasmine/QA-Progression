July 5, 2025
Why is Testing Necessary?
 From the user’s perspective:
It is everywhere
Software that has minor issues might cause huge problems and frustrations
Can cause:
Money loss
Waste of time and effort
Business reputation
Loss of life
No issue is small

From the project management perspective:
It is a form of quality control

Typical Objectives in Testing:
Evaluating work products such as user requirements, user stories , designs and code
Causing failures and finding defects
Ensuring required coverage of a test object
Reducing the risk level of inadequate software quality
Verifying whether specified requirements have been fulfilled
Verifying that a test object complies with contractual, legal, and regulatory requirements
Providing information to stake holders to allow them to make informed decisions
Building confidence in the quality of a test object
Validating whether the test object is complete and works as expected by the stakeholders
Test objectives  are usually straight forward 
Test objectives display a list of time and ask what is a test objectives and what is not

Validation vs Verification
Validation mentions the user more and asks if we are building the right product
Verification mentions the documents more and asks if are we building the product right

Errors, Defects and Failures
A person makes an error that creates a defect / bug in the software that can cause a failure in the system operation
Example:
Someone has a fever (failure, a symptom you can see) and goes to the Dr who decides something is wrong with the stomach (a defect) because the patient ate too much (error)
Failures can also be caused by environmental conditions

False-positive vs False-negative
False-positive
We did something wrong which we thought we found a defect but it was a wrong finding
Probably duet to:
Errors in the test execution
Defects in the test data
Test environments
Other reasons
False-negative
We did something wrong but we cannot find the defect

Root Cause Analysis 
Helps in preventive measures for the software in the near future

Root Cause, Error, Defect, Failure
Root causes leads to human error which then leads to the defect thus the failure
Example:
The designer is tired (root cause)
Designer documents wrong for disabled users (error)
The programmers are in a severe time pressure (root cause)
So they did not include exception handling for calculations (error)

Dynamic and Static Testing
Dynamic testing is testing that occurs once the system is already there, so expect that there will be defects
Static testing takes place early in the software life cycle
Includes techniques such as reviewing documents and prevents defects from being introduced into the code

**July 6, 2025**

**Verification and Validation**

- Verification is whether the system the specified documents
- Validation checks whether the system will meet user and other stakeholder needs
- Verification + validation = doing the right thing in the right way

**What is Testing?**

- Testing is a big industry and has many branches
- Set of activities to discover defects and evaluate the quality of software artifacts
- **Artifacts are anything the development and testing teams produce to help create the software product**
- **Test objects are any artifact that is being tested**

**Testing and Debugging**

- Testing finds defects
- Debugging fixes these defects (a development activity)
- Debugging activities include:
    - Reproduction of a failure
    - Diagnosis (finding the root cause)
    - Fixing the cause
- Confirmation testing or re-testing is to test to check whether the bug was fixed
- Static testing points to the defect directly, debugging is concerned with removing it
    - No need for reproduction or diagnosis since static testing directly finds defects and cannot cause failures

**Does testing increase the quality of the software?**

- **No. Fixing bugs is a development activity not a testing activity. So testing simply builds confidence in the quality of the test object by providing higher-quality test objects**

**Testing’s Contributions to Success**

- Provides a cost-effective means of detecting defects

**Quality Assurance and testing**

- **Quality assurance -** “process-oriented”. The better the process, the better the software. “Process improvements”
    - If a good process is followed correctly, then it will generate a good product
    - A preventive approach
    - Test results in QA provide feedback on how well the development and processes perform
- **Testing -** “product-oriented”. It is a major form of **quality control**.
    - A corrective approach
    - Test results for QC are used to fix defects

**July 8, 2025**

**The Concept of Coverage in Software Testing**

- Test coverage is an essential part of software testing
- Measures the amount of testing performed by a set of tests
- Test coverage measures the effectiveness of our testing
- Parts we can measure for coverage
    - Requirements Coverage = test against all requirements
    - Structural coverage = has each design been exercised during testing
    - Implementation coverage = has each line of code been exercised?
- How can we know which requirement has been tested?
    - Traceability matrix

**The Seven Testing Principles**

1. Testing shows the presence of defects, not their absence
    1. There is no such thing as bug-free software
2. Exhaustive testing is impossible
    1. Testing everything is impossible except for trivial cases
3. Early Testing Saves Time and Money
    1. Early testing would be cheaper
    2. Time and Effort is reduced if discovered in the early phase
4. Defects Cluster Together
    1. A small number of modules usually contains most of the defects
    2. Closely related to the pareto principle (80/20 principle)
        1. 80 percent of the problems are usually found in 20% of the modules
5. Tests wear out
    1. If same tests are repeated, eventually the same set of test cases will no longer find any new defects
6. Testing is context-dependent
    1. Different testing is necessary for different circumstances
    2. Testing in an agile is different in a sequential
7. Absence of errors is a fallacy
    1. A mistake that there is no error present

**Test Conditions, Test Cases, Test procedure, and Test Suites**

- **Test Condition** is an item or event of a component that can be verified by one or more test cases.
    - Example: a function, transaction, feature, quality, etc
- **Test Cases is a set of input values, preconditions, expected results and post conditions developed from a test condition or objective**
    - **High-level test cases** mean they don't indicate exact data, just logical information
    - **Low-level test cases are also called concrete test cases (e.g. = test with input 10). Are test cases that contain data**
- **Test Procedure  - steps to execute a test case**
- **Test Suite -  Categorizing test procedures in such a way they match your planning and analysis needs.**

**Test Activities, Testware and Test Roles**

- ISO/IEC/IEEE 29119-2 describes the test processes in ISO Standard
- Testing is a process not simply just testing

**Test Activities and Tasks**

- Test process consists of the 7 groups of activities:
1. Test Planning
2. Test Monitoring and Control
3. Test Analysis
4. Test Design
5. Test Implementation
6. Test Execution
7. Test Completion
- Each activity may contain several more activities which may contain one or more tasks
- Test activities are organized and carried out differently in different life cycles
- Are usually implemented iteratively, not sequentially

**1. Test Planning**

- Define the objectives of testing
- Decide what to test
- Who will do the test
- How will they do the testing
- Define specific test activities to meet the objectives
- Define when we can consider the testing complete, called the **exit criteria**

**2. Test Monitoring and Control**

- The ongoing activity of comparing actual progress against the test plan using test monitoring metrics defined in the test plan
- Here is where we evaluate the exit criteria
- Evaluating exit criteria is the activity where test execution results are assessed against the defined objectives

**3. Test Analysis**

- Knowing what to test and breaking it into test conditions
- Any info or documentation we have is analyzed to identify testable features and define test conditions
- **Test Basis  - any document we can use as a reference**
    - Requirements specifications, such as BRD, SRS, user stories, use cases, etc.
    - Design and implementation information, UML, UI, etc
    - Code itself
    - Risk Analysis reports
- Identify features and sets of features to be tested
- Define and prioritize the test conditions for each features based on analysis
- Capture traceability = meaning we have test conditions for all features we have decided to test

**4. Test Design**

- Test design answers the question how to test
- Designing sets of test cases and prioritizing them
- Identifying test data to support test conditions
- Designing the test environment
- Capture bi-directional traceability

**5. Test Implementation**

- The testware necessary for test execution is created during test implementation
- “Do we now have everything in place to run the tests?”
- Create and implement test procedures during test implementation
- Prioritizing the test procedures
- Creating the test suites and arranging them
- Building the test environment
- Prepare and implement test data
- Verifying the bi-directional traceability

**Note:**

Test conditions = test analysis

Test Cases = test design

Test procedures = test implementation

Design data = test design

Implement the data = test implementation

**6. Test Execution**

- Test suites are run during the execution following the schedule
- Keeping a log of the testing and testware (pass or fail)
- Run test cases in order manually or automated
- Comparing actual results with expected results
- Analyzing anomalies when there is a difference between actual and expected results
- Reporting defects to devs for fix
- Confirmation testing
- Verifying and updating traceability

**7. Test Completion**

- Occur at project milestones
- Collect data from completed tests to consolidate experience
- Checking deliverables if they have been delivered
- Documentation is in order
- Create test summary report
- Check whether all defect reports are don
- Make sure delete confidential data
- Handing over the testware to the maintenance team
- Analyzing lessons
- Using data for improvement of test process maturity

**July 11, 2025**

**Test Process in Context**

- Not performed in isolation
- Organizational software development processes typically include testing activities
- Stakeholders provide financial funding for testing
- Test depends on context
- Factors Influencing Test Processes:
1. Stakeholders
2. Team Members
3. Business Domain
4. Technical Factors
5. Project Constraints
6. Organizational Factors
7. SDLC
8. Tools

**Testware**

- Any output work products like documents, reports, and lists created as part of the test processes
- Test process organization variation implementation:
1. Types of work products created per test process
2. Ways those work products are organized and managed
3. Names used for these work products
- Testing Standards:
1. ISO/IEC/IEEE-29119-1
- Software Testing Concepts
1. ISO/IEC/IEEE-29119-2

-	Software Testing Process

1. ISO/IEC/IEEE-29119-3
- Test Work Products
- 7 Different Work Products Per Different Test Process

**Traceability Between Test Basis and Test Work Products**

- Traceability of the test cases to requirements can verify if the requirements are covered by test cases
- Good Traceability Supports:
1. Analyzing Impact Changes
2. Meeting IT governance criteria
3. Making testing auditable
4. Improving the understandability of test progress reports and test summary reports
5. Relate IT terms and testing to stakeholders so they can understand them
6. Providing information about progress
- **Note:**
    - Traceability has nothing to do in any kind of estimation
    - Also has nothing to do with selecting data
    - It also cannot determine risk level using traceability

**Roles in Testing**

- 2 Main Roles
1. Test Manager
- Test process management
- Test planning and monitoring, etc
- “How-to-do” things
- **Test processes: Test planning, monitoring and control, Test completion**
- Test coach -> outside of the team
1. Tester
- Overall responsibility is the technical aspect of testing
- “Hands-on” to do things
- **Test processes: Test analysis, Test design, Test implementation, Test execution**

**Essential Skills and Good Practices in Testing**

- Knowledge + Practice + Aptitude (natural talent)
1. Testing Knowledge
2. Thoroughness, carefulness, etc, attention to detail
3. Good communication skills
4. Analytical Thinking
5. Technical Knowledge
6. Domain Knowledge

**Communication Skills for Testers**

- Developers treat their codes as if its their kids. They don't take criticism lightly

**Whole Team Approach**

- Ability for a tester to work in a team
- Contribute positively to team goals
- Any team member with the necessary knowledge and skills can perform any task and everyone is responsible for quality
- The team members share the same workspace
- Team takes responsibility in all kinds of testing tasks
- It generates team synergy that benefits the entire project
- Improve team dynamics
- Improve communication between team members

**Independence of Testing**

- Independence is not a replacement for familiarity
- Degrees of Independent Testing (from dependence 1 to independence 5)
1. The developer
2. Tester from the developer team
3. Independent test team
4. Customer of specialist testers
5. Outsourced test teams or testers
- Pros:
1. Independent testers are likely to recognize different kinds of failures compared to developers
2. They can verify, challenge or disprove assumptions
- Cons:
1. More isolation from the development team
2. Developers may lose a sense of responsibility for quality
3. Maybe seen for bottleneck or blamed for delays in release
4. May lack important information

July 13, 2025
Section 2 talks about:
How testing is incorporated in other dev activities
Different Levels and types of testing
Maintenance Testing
SDLC Models
How activities are organized to create the software
Vary according to:
Objective
Discipline
Interest
Time to market
Documentation
Types of activity performed at each stage
Others
Testing depends on the SDLC model
Categories of SDLC Models
Sequential Development Models
Linear with sequential flow of activities
No overlaps of phases
Iterative &Incremental Development Models
Short cycle of activities that can be revisited any time
Sequential Development Models: Waterfall
Oldest and well known
Each stage must be completed before moving on to the next stage
Requirements Phase
From the user
Systems specification document
Functional specifications
Design Phase
Global design or architecture design or high-level design to map the system requirements
Detailed design or low-level design: very specific about the functions
Coding Phase
Developers code the detailed design of each module to software
Testing Phase
Towards the end of the project lifecycle
*Waterfall model is only a good model for very specific circumstances
July 14, 2025
V-Model
A sequential model
Testing needs to begin as early as possible in the lifecycle
The model integrates testing throughout the entire development phase
The 4 Test Models in V-model
Component Testing
Search defects in implemented components
Integration Testing
Test interfaces and interactions between different components
System Testing
Behavior of the whole system
Acceptance Testing
Conducted whether or not to accept the system
E.g client UAT

