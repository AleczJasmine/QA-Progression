July 5, 2025
Why is Testing Necessary?
 From the user’s perspective:
It is everywhere
Software that has minor issues might cause huge problems and frustrations
Can cause:
Money loss
Waste of time and effort
Business reputation
Loss of life
No issue is small

From the project management perspective:
It is a form of quality control

Typical Objectives in Testing:
Evaluating work products such as user requirements, user stories , designs and code
Causing failures and finding defects
Ensuring required coverage of a test object
Reducing the risk level of inadequate software quality
Verifying whether specified requirements have been fulfilled
Verifying that a test object complies with contractual, legal, and regulatory requirements
Providing information to stake holders to allow them to make informed decisions
Building confidence in the quality of a test object
Validating whether the test object is complete and works as expected by the stakeholders
Test objectives  are usually straight forward 
Test objectives display a list of time and ask what is a test objectives and what is not

Validation vs Verification
Validation mentions the user more and asks if we are building the right product
Verification mentions the documents more and asks if are we building the product right

Errors, Defects and Failures
A person makes an error that creates a defect / bug in the software that can cause a failure in the system operation
Example:
Someone has a fever (failure, a symptom you can see) and goes to the Dr who decides something is wrong with the stomach (a defect) because the patient ate too much (error)
Failures can also be caused by environmental conditions

False-positive vs False-negative
False-positive
We did something wrong which we thought we found a defect but it was a wrong finding
Probably duet to:
Errors in the test execution
Defects in the test data
Test environments
Other reasons
False-negative
We did something wrong but we cannot find the defect

Root Cause Analysis 
Helps in preventive measures for the software in the near future

Root Cause, Error, Defect, Failure
Root causes leads to human error which then leads to the defect thus the failure
Example:
The designer is tired (root cause)
Designer documents wrong for disabled users (error)
The programmers are in a severe time pressure (root cause)
So they did not include exception handling for calculations (error)

Dynamic and Static Testing
Dynamic testing is testing that occurs once the system is already there, so expect that there will be defects
Static testing takes place early in the software life cycle
Includes techniques such as reviewing documents and prevents defects from being introduced into the code

**July 6, 2025**

**Verification and Validation**

- Verification is whether the system the specified documents
- Validation checks whether the system will meet user and other stakeholder needs
- Verification + validation = doing the right thing in the right way

**What is Testing?**

- Testing is a big industry and has many branches
- Set of activities to discover defects and evaluate the quality of software artifacts
- **Artifacts are anything the development and testing teams produce to help create the software product**
- **Test objects are any artifact that is being tested**

**Testing and Debugging**

- Testing finds defects
- Debugging fixes these defects (a development activity)
- Debugging activities include:
    - Reproduction of a failure
    - Diagnosis (finding the root cause)
    - Fixing the cause
- Confirmation testing or re-testing is to test to check whether the bug was fixed
- Static testing points to the defect directly, debugging is concerned with removing it
    - No need for reproduction or diagnosis since static testing directly finds defects and cannot cause failures

**Does testing increase the quality of the software?**

- **No. Fixing bugs is a development activity not a testing activity. So testing simply builds confidence in the quality of the test object by providing higher-quality test objects**

**Testing’s Contributions to Success**

- Provides a cost-effective means of detecting defects

**Quality Assurance and testing**

- **Quality assurance -** “process-oriented”. The better the process, the better the software. “Process improvements”
    - If a good process is followed correctly, then it will generate a good product
    - A preventive approach
    - Test results in QA provide feedback on how well the development and processes perform
- **Testing -** “product-oriented”. It is a major form of **quality control**.
    - A corrective approach
    - Test results for QC are used to fix defects

**July 8, 2025**

**The Concept of Coverage in Software Testing**

- Test coverage is an essential part of software testing
- Measures the amount of testing performed by a set of tests
- Test coverage measures the effectiveness of our testing
- Parts we can measure for coverage
    - Requirements Coverage = test against all requirements
    - Structural coverage = has each design been exercised during testing
    - Implementation coverage = has each line of code been exercised?
- How can we know which requirement has been tested?
    - Traceability matrix

**The Seven Testing Principles**

1. Testing shows the presence of defects, not their absence
    1. There is no such thing as bug-free software
2. Exhaustive testing is impossible
    1. Testing everything is impossible except for trivial cases
3. Early Testing Saves Time and Money
    1. Early testing would be cheaper
    2. Time and Effort is reduced if discovered in the early phase
4. Defects Cluster Together
    1. A small number of modules usually contains most of the defects
    2. Closely related to the pareto principle (80/20 principle)
        1. 80 percent of the problems are usually found in 20% of the modules
5. Tests wear out
    1. If same tests are repeated, eventually the same set of test cases will no longer find any new defects
6. Testing is context-dependent
    1. Different testing is necessary for different circumstances
    2. Testing in an agile is different in a sequential
7. Absence of errors is a fallacy
    1. A mistake that there is no error present

**Test Conditions, Test Cases, Test procedure, and Test Suites**

- **Test Condition** is an item or event of a component that can be verified by one or more test cases.
    - Example: a function, transaction, feature, quality, etc
- **Test Cases is a set of input values, preconditions, expected results and post conditions developed from a test condition or objective**
    - **High-level test cases** mean they don't indicate exact data, just logical information
    - **Low-level test cases are also called concrete test cases (e.g. = test with input 10). Are test cases that contain data**
- **Test Procedure  - steps to execute a test case**
- **Test Suite -  Categorizing test procedures in such a way they match your planning and analysis needs.**

**Test Activities, Testware and Test Roles**

- ISO/IEC/IEEE 29119-2 describes the test processes in ISO Standard
- Testing is a process not simply just testing

**Test Activities and Tasks**

- Test process consists of the 7 groups of activities:
1. Test Planning
2. Test Monitoring and Control
3. Test Analysis
4. Test Design
5. Test Implementation
6. Test Execution
7. Test Completion
- Each activity may contain several more activities which may contain one or more tasks
- Test activities are organized and carried out differently in different life cycles
- Are usually implemented iteratively, not sequentially

**1. Test Planning**

- Define the objectives of testing
- Decide what to test
- Who will do the test
- How will they do the testing
- Define specific test activities to meet the objectives
- Define when we can consider the testing complete, called the **exit criteria**

**2. Test Monitoring and Control**

- The ongoing activity of comparing actual progress against the test plan using test monitoring metrics defined in the test plan
- Here is where we evaluate the exit criteria
- Evaluating exit criteria is the activity where test execution results are assessed against the defined objectives

**3. Test Analysis**

- Knowing what to test and breaking it into test conditions
- Any info or documentation we have is analyzed to identify testable features and define test conditions
- **Test Basis  - any document we can use as a reference**
    - Requirements specifications, such as BRD, SRS, user stories, use cases, etc.
    - Design and implementation information, UML, UI, etc
    - Code itself
    - Risk Analysis reports
- Identify features and sets of features to be tested
- Define and prioritize the test conditions for each features based on analysis
- Capture traceability = meaning we have test conditions for all features we have decided to test

**4. Test Design**

- Test design answers the question how to test
- Designing sets of test cases and prioritizing them
- Identifying test data to support test conditions
- Designing the test environment
- Capture bi-directional traceability

**5. Test Implementation**

- The testware necessary for test execution is created during test implementation
- “Do we now have everything in place to run the tests?”
- Create and implement test procedures during test implementation
- Prioritizing the test procedures
- Creating the test suites and arranging them
- Building the test environment
- Prepare and implement test data
- Verifying the bi-directional traceability

**Note:**

Test conditions = test analysis

Test Cases = test design

Test procedures = test implementation

Design data = test design

Implement the data = test implementation

**6. Test Execution**

- Test suites are run during the execution following the schedule
- Keeping a log of the testing and testware (pass or fail)
- Run test cases in order manually or automated
- Comparing actual results with expected results
- Analyzing anomalies when there is a difference between actual and expected results
- Reporting defects to devs for fix
- Confirmation testing
- Verifying and updating traceability

**7. Test Completion**

- Occur at project milestones
- Collect data from completed tests to consolidate experience
- Checking deliverables if they have been delivered
- Documentation is in order
- Create test summary report
- Check whether all defect reports are don
- Make sure delete confidential data
- Handing over the testware to the maintenance team
- Analyzing lessons
- Using data for improvement of test process maturity
